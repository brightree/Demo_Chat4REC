# ë³€ê²½ ì‚¬í•­
    # multi_agent_chatbot

import streamlit as st
from dotenv import load_dotenv
import os
import json
import uuid
from datetime import datetime
from typing import Literal, TypedDict
import openai
from langgraph.graph import StateGraph
from supabase import create_client
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, OpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.chains import RetrievalQA
#from langchain_community.llms import OpenAI
#from langchain_community.embeddings import OpenAIEmbeddings

# ==========================
# ğŸ”§ í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================
load_dotenv()
api_key = os.getenv("MY_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not api_key:
    st.error("â—OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

client = openai.OpenAI(api_key=api_key)
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ===============================
# ğŸ§± ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ===============================
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "conversation_id" not in st.session_state:
    st.session_state.conversation_id = f"conv_{uuid.uuid4().hex[:8]}"
if "turn_index" not in st.session_state:
    st.session_state.turn_index = 0

# ===========================
# ğŸ“š ê°•ì˜ ë°ì´í„° ë¡œë”© (Agent2)
# ===========================
@st.cache_data
def load_course_data():
    try:
        with open("sales_learning_dummy_data.json", "r", encoding="utf-8") as f:
            return json.load(f)["courses"]
    except Exception as e:
        st.error(f"â—ê°•ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
        return []

course_data = load_course_data()

# ===========================
# ğŸ“ PDF ë¬¸ì„œ ì„ë² ë”© (Agent1)
# ===========================
@st.cache_resource
def load_or_create_rag_retriever(
    file_path: str,
    index_dir: str = "faiss_index",
    chunk_size: int = 700,
    chunk_overlap: int = 150,
) -> FAISS:
    if os.path.exists(index_dir):
        return FAISS.load_local(
            index_dir,
            OpenAIEmbeddings(api_key=api_key),
            allow_dangerous_deserialization=True  # âœ… ì—¬ê¸°ê°€ í•µì‹¬ì…ë‹ˆë‹¤
        ).as_retriever()


    # 1. PDF ì½ê¸° ë° ì²­í‚¹
    loader = PyMuPDFLoader(file_path)
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    chunks = splitter.split_documents(documents)

    # 2. ì„ë² ë”© ë° FAISS ì €ì¥
    embeddings = OpenAIEmbeddings(api_key=api_key)
    db = FAISS.from_documents(chunks, embeddings)
    db.save_local(index_dir)
    return db.as_retriever()

rag_retriever = load_or_create_rag_retriever("Rag_Galaxy25_Ultra.pdf")
rag_chain = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), retriever=rag_retriever)

# ==============================
# ğŸ§  LangGraph ìƒíƒœ ì •ì˜
# ==============================
class GraphState(TypedDict):
    user_query: str
    final_response: str
    route: Literal["agent1", "agent2"]

# ==============================
# ğŸ” ì˜ë„ ë¶„ë¥˜ ì—”ì§„ ë…¸ë“œ
# ==============================
def route_intent(state: GraphState) -> GraphState:
    user_query = state["user_query"]

    routing_prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì–´ë–¤ ìœ í˜•ì¸ì§€ ë¶„ë¥˜í•´ ì£¼ì„¸ìš”.
ê° ìœ í˜• ë³„ë¡œ ë‹¤ë¥¸ LLM Agentë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, í•©ë¦¬ì  ë¶„ë¥˜ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.

- ì œí’ˆ ìŠ¤í™, ê¸°ëŠ¥, ë°°í„°ë¦¬, ì¹´ë©”ë¼ ë“± 'ì œí’ˆ ì •ë³´'ì— ëŒ€í•œ ì§ˆë¬¸ì´ë©´: agent1
- ì˜ì—… ê³ ë¯¼, ê°•ì˜ ì¶”ì²œ, í•™ìŠµì— ëŒ€í•œ ì§ˆë¬¸ì´ë©´: agent2

â—ï¸ë°˜ë“œì‹œ ì•„ë˜ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ë”°ì˜´í‘œ ì—†ì´ ë‹¨ë…ìœ¼ë¡œ í•œ ì¤„):
agent1
agent2

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_query}
"""

    response = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."},
            {"role": "user", "content": routing_prompt}
        ]
    )

    raw = response.choices[0].message.content.strip().lower()

    # ì•ˆì •ì  ì²˜ë¦¬
    if "agent1" in raw:
        route = "agent1"
    elif "agent2" in raw:
        route = "agent2"
    else:
        # fallback: í•™ìŠµ ê´€ë ¨ìœ¼ë¡œ ì²˜ë¦¬
        route = "agent2"

    return {**state, "route": route}

# ==============================
# ğŸ¤– Agent1 (RAG ê¸°ë°˜ ì œí’ˆ ë‹µë³€)
# ==============================
def agent1_product_info(state: GraphState) -> GraphState:
    user_query = state["user_query"]
    try:
        answer = rag_chain.run(user_query)
    except Exception as e:
        answer = f"â—ì œí’ˆ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    return {**state, "final_response": answer}

# ==============================
# ğŸ“ Agent2 (ê°•ì˜ ì¶”ì²œ ì±—ë´‡)
# ==============================
def agent2_recommend_courses(state: GraphState) -> GraphState:
    full_history = ""
    for turn in st.session_state.chat_history:
        full_history += f"ì‚¬ìš©ì: {turn['user']}\n"
        full_history += f"ì±—ë´‡: {turn['bot']}\n"
    full_history += f"ì‚¬ìš©ì: {state['user_query']}\n"

    prompt = f"""
ë‹¹ì‹ ì€ ì‚¼ì„±ì „ì ì˜ì—…ì‚¬ì›ì„ ìœ„í•œ "ì „ë¬¸ ê°•ì˜ ì¶”ì²œ ì±—ë´‡"ì…ë‹ˆë‹¤.

## ì—­í• 
- ì‚¼ì„±ì „ì ì˜ì—…ì‚¬ì›ì´ ì œí’ˆ ì„¤ëª…, ê³ ê° ìƒë‹´, ì„¸ì¼ì¦ˆ ì „ëµ ë“± ì—…ë¬´ì—ì„œ ê²ªëŠ” ê³ ë¯¼ì„ ê¸°ë°˜ìœ¼ë¡œ, ê°€ì¥ ì í•©í•œ ê°•ì˜ë¥¼ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
- ì‚¬ìš©ìëŠ” ë°”ìœ ì—…ë¬´ ì¤‘ ì§ˆë¬¸í•˜ë¯€ë¡œ, ì‘ë‹µì€ í•­ìƒ **í•µì‹¬ì ì´ê³  ê°„ê²°í•˜ê²Œ**, **ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ ì–´íˆ¬**ë¡œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.

## ëª©í‘œ
- ì‚¬ìš©ìì˜ ê³ ë¯¼/ìƒí™©/ì„ í˜¸ ìŠ¤íƒ€ì¼ì„ íŒŒì•…í•˜ì—¬ ë§ì¶¤ ê°•ì˜ 3~5ê°œë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
- ë‹¨, ì‚¬ìš©ìê°€ "ì§€ê¸ˆ ë°”ë¡œ ì¶”ì²œí•´ì¤˜"ë¼ê³  ë§í•˜ì§€ ì•ŠëŠ” ì´ìƒ, 2íšŒ ë‚´ì™¸ì˜ ìƒí˜¸ëŒ€í™”ë¥¼ í†µí•´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - ì˜ˆë¥¼ ë“¤ì–´, ë‹¨ìˆœí•œ ì¸ì‚¿ë§ë§Œ ì£¼ê³  ë°›ì•˜ë‹¤ë©´ ì›í•˜ëŠ” ê°•ì˜ë‚˜ í•™ìŠµ ê³ ë¯¼ì„ ìœ ë„ ì§ˆë¬¸í•˜ì—¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

## ëŒ€í™” íë¦„ ì§€ì¹¨ (Flow Logic)
1. **ì •ë³´ ìˆ˜ì§‘**
   - ì‚¬ìš©ìì˜ ì‘ë‹µì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê²Œ ì§ˆë¬¸ì„ ì´ì–´ê°€ë©°, í•œ ë²ˆì— í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ ë˜ì§‘ë‹ˆë‹¤.
   - ì§ˆë¬¸ì€ ì§§ê³  ëª…í™•í•˜ë˜, ì ì ˆí•œ ê°•ì˜ë¥¼ ì¶”ì²œí•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸í•©ë‹ˆë‹¤.

2. **ì¦‰ì‹œ ì¶”ì²œ ì˜ˆì™¸**
   - ì‚¬ìš©ìê°€ â€œì§€ê¸ˆ ë°”ë¡œ ì¶”ì²œí•´ì¤˜â€ë¼ê³  ë§í•˜ë©´, ì§ˆë¬¸ ìƒëµ í›„ ë°”ë¡œ ì¶”ì²œì„ ì§„í–‰í•©ë‹ˆë‹¤.

3. **ì¶”ì²œ ì‘ë‹µ í˜•ì‹**
   - ê°•ì˜ëŠ” ë‹¤ìŒ ì •ë³´ë¥¼ ë‹´ë˜, ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤:
     - ê°•ì˜ ì œëª©
     - ì¶”ì²œ ì´ìœ  (ì‚¬ìš©ì ê³ ë¯¼ê³¼ ì—°ê²°)
     - ë§í¬: https://www.ubion.co.kr/ubion/
   - ì¶”ì²œ ê°•ì˜ëŠ” **ì¤‘ìš”ë„ ìˆœì„œë¡œ 3~5ê°œ**, í•­ëª©ë³„ë¡œ êµ¬ë¶„í•´ ì œì‹œí•©ë‹ˆë‹¤.

4. **ì˜ˆì™¸ ì‘ëŒ€**
   - ì‚¬ìš©ìê°€ ë†ë‹´, ê³¼ê²©í•œ í‘œí˜„, ìš•ì„¤ ë“±ì„ ì…ë ¥í•˜ë”ë¼ë„ ê°ì •ì  ëŒ€ì‘ ì—†ì´ ì›ë˜ ì£¼ì œë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„í•©ë‹ˆë‹¤.

## ì‘ë‹µ ìŠ¤íƒ€ì¼ (Tone & Format)
- ë§íˆ¬: ì „ë¬¸ì„± ìˆê³  ì¹œì ˆí•œ ë§íˆ¬
- ë¬¸ì¥: ì§§ê³  í•µì‹¬ì ìœ¼ë¡œ, ì •ë³´ ë°€ë„ ë†’ê²Œ
- ë¶ˆí•„ìš”í•œ ì„œë‘/ê²°ë¡ /ê³¼ì‰ ì„¤ëª… ë°°ì œ
- ì‚¬ìš©ìì˜ í‘œí˜„ì„ ìš”ì•½í•˜ë©° ë‹¤ìŒ ì§ˆë¬¸ ë˜ëŠ” ì¶”ì²œìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°

## ì‘ë‹µ ì˜ˆì‹œ (ì°¸ê³ ìš© í”„ë¼ì´ë°)
ì‚¬ìš©ì: ê³ ê° ì‘ëŒ€ê°€ í˜ë“¤ì–´ìš”  
â†’ ì±—ë´‡: ê³ ê° ëŒ€ì‘ì´ ì–´ë ¤ìš°ì‹œêµ°ìš”. í˜¹ì‹œ ì–´ë–¤ ìœ í˜•ì˜ ê³ ê°ì´ íŠ¹íˆ ì–´ë ¤ìš°ì…¨ë‚˜ìš”? ì˜ˆë¥¼ ë“¤ë©´ ë¶ˆë§Œ ê³ ê°, ì„¤ëª…ì„ ëª» ì•Œì•„ë“£ëŠ” ê³ ê° ë“±ìš”.  

ì‚¬ìš©ì: ì§‘ì¤‘ì´ ì˜ ì•ˆë¼ìš”.
â†’ ì±—ë´‡: ì§§ê³  í•µì‹¬ì ì¸ ê°•ì˜ë¥¼ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”.
â†’ [ê°•ì˜ ì¶”ì²œ ì‹œì‘]


[ëŒ€í™” ê¸°ë¡]
{full_history}

ì´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ì§ˆë¬¸ì´ í•„ìš”í•˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ê³ ,  
ì •ë³´ê°€ ì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ ê°•ì˜ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.


[ê°•ì˜ ëª©ë¡]
{json.dumps(course_data, ensure_ascii=False, indent=2)}
"""
    try:
        res = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[
                {"role": "system", "content": "ì‚¼ì„±ì „ì ì„¸ì¼ì¦ˆ ê°•ì˜ ì¶”ì²œ ì „ë¬¸ê°€"},
                {"role": "user", "content": prompt}
            ]
        )
        response_text = res.choices[0].message.content.strip()
    except Exception as e:
        response_text = f"â—ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    return {**state, "final_response": response_text}

# ==============================
# ğŸ” LangGraph êµ¬ì¶•
# ==============================
builder = StateGraph(GraphState)
builder.add_node("route_intent", route_intent)
builder.add_node("agent1", agent1_product_info)
builder.add_node("agent2", agent2_recommend_courses)

builder.set_entry_point("route_intent")
builder.add_conditional_edges("route_intent", lambda x: x["route"], {
    "agent1": "agent1",
    "agent2": "agent2"
})

graph = builder.compile()

# ==============================
# ğŸ’¾ Supabase ì €ì¥ í•¨ìˆ˜
# ==============================
def save_chat_to_db(user_input, llm_response):
    supabase.table("chat_history").insert({
        "user_id": "guest_user",
        "conversation_id": st.session_state.conversation_id,
        "turn_index": st.session_state.turn_index,
        "timestamp": datetime.utcnow().isoformat(),
        "user_input": user_input,
        "llm_response": llm_response
    }).execute()
    st.session_state.turn_index += 1

# ==============================
# ğŸ¨ Streamlit UI
# ==============================
st.set_page_config(
    page_title="ì‚¼ì„± ì„¸ì¼ì¦ˆ Agentic ì±—ë´‡",
    page_icon="ğŸ’¼",
    layout="centered",
    initial_sidebar_state="collapsed"
)

for turn in st.session_state.chat_history:
    st.chat_message("user").write(turn["user"])
    st.chat_message("assistant").write(turn["bot"])

samsung_blue = "#1428A0"
st.markdown(f"""
    <div style="text-align:center; margin-bottom:20px;">
        <img src="https://upload.wikimedia.org/wikipedia/commons/2/24/Samsung_Logo.svg" width="180" />
        <h2 style="color:{samsung_blue};">ì‚¼ì„±ì „ì Sales Agentic Assitant</h2>
        <p style="font-size:15px;">ì œí’ˆ ìŠ¤í™ ì •ë³´ë¶€í„° ê³ ê° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê³ ë¯¼ê¹Œì§€, ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!</p>
    </div>
""", unsafe_allow_html=True)

user_input = st.chat_input("ì„¸ì¼ì¦ˆ ê´€ë ¨ ê¶ê¸ˆì ì„ ë§ì”€í•´ ì£¼ì„¸ìš”.")
if user_input:
    st.chat_message("user").write(user_input)
    result = graph.invoke({"user_query": user_input})
    response_text = result["final_response"]
    
    # âœ… í˜„ì¬ ì‘ë‹µí•œ agent ì•Œë ¤ì£¼ê¸°
    route_used = result.get("route", "")
    if route_used == "agent1":
        response_header = "ğŸ“± [ì œí’ˆ ì •ë³´ Agent]"
    elif route_used == "agent2":
        response_header = "ğŸ“ [í•™ìŠµ ì¶”ì²œ Agent]"
    else:
        response_header = "ğŸ¤– [Agent ì‘ë‹µ]"

    st.chat_message("assistant").write(f"{response_header}\n\n{response_text}")
    
    st.session_state.chat_history.append({"user": user_input, "bot": f"{response_header}\n\n{response_text}"})
    save_chat_to_db(user_input, response_text)
