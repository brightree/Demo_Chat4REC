# ë³€ê²½ ì‚¬í•­
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©

# ==========================
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ==========================
import os
import json
import uuid
from datetime import datetime, timezone
from typing import Literal, TypedDict

# ==========================
# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ==========================
from dotenv import load_dotenv
import openai
from supabase import create_client

# ==========================
# LangChain ê´€ë ¨
# ==========================
import streamlit as st
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
# LangChain Community ëª¨ë“ˆ
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyMuPDFLoader
# LangChain OpenAI ê´€ë ¨
from langchain_openai import OpenAIEmbeddings, OpenAI
# LangGraph
from langgraph.graph import StateGraph

# ==========================
# ğŸ”§ í™˜ê²½ ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================
load_dotenv()
api_key = os.getenv("MY_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

if not api_key:
    st.error("â—OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

client = openai.OpenAI(api_key=api_key)
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ===============================
# ğŸ§± ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ===============================
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "conversation_id" not in st.session_state:
    st.session_state.conversation_id = f"conv_{uuid.uuid4().hex[:8]}"
if "turn_index" not in st.session_state:
    st.session_state.turn_index = 0

# ===========================
# ğŸ“ PDF ë¬¸ì„œ ì„ë² ë”© (Agent1)
# ===========================
@st.cache_resource
def load_or_create_rag_retriever(
    file_path: str,
    index_dir: str = "faiss_index",
    chunk_size: int = 700,
    chunk_overlap: int = 150,
) -> FAISS:
    if os.path.exists(index_dir):
        return FAISS.load_local(
            index_dir,
            OpenAIEmbeddings(api_key=api_key),
            allow_dangerous_deserialization=True  # âœ… ì—¬ê¸°ê°€ í•µì‹¬ì…ë‹ˆë‹¤
        ).as_retriever()


    # 1. PDF ì½ê¸° ë° ì²­í‚¹
    loader = PyMuPDFLoader(file_path)
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )
    chunks = splitter.split_documents(documents)

    # 2. ì„ë² ë”© ë° FAISS ì €ì¥
    embeddings = OpenAIEmbeddings(api_key=api_key)
    db = FAISS.from_documents(chunks, embeddings)
    db.save_local(index_dir)
    return db.as_retriever()

rag_retriever = load_or_create_rag_retriever("RAG/Rag_Galaxy25_Ultra.pdf")
rag_chain = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), retriever=rag_retriever)

# ===========================
# ğŸ“š ê°•ì˜ ë°ì´í„° ì „ì²˜ë¦¬ (Agent2)
# ===========================

# ê°•ì˜ ë°ì´í„° ì½ê¸°
@st.cache_data
def load_course_data():
    try:
        with open("RAG/sales_learning_dummy_data.json", "r", encoding="utf-8") as f:
            return json.load(f)["courses"]
    except Exception as e:
        st.error(f"â—ê°•ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {e}")
        return []

course_data = load_course_data()

# ê°•ì˜ ë°ì´í„° ë­ì²´ì¸ ë¬¸ì„œë¡œ ë³€í™˜
def course_data_to_documents(course_data: list) -> list[Document]:
    docs = []
    for course in course_data:
        text = "\n".join([f"{key}: {value}" for key, value in course.items()])
        docs.append(Document(page_content=text, metadata={"title": course.get("title", "")}))
    return docs

# ì„ë² ë”© ë° ì²­í‚¹
@st.cache_resource
def create_course_rag_retriever(course_data: list, index_dir: str = "course_faiss_index") -> FAISS:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

    if os.path.exists(index_dir):
        return FAISS.load_local(
            index_dir,
            OpenAIEmbeddings(api_key=api_key),
            allow_dangerous_deserialization=True
        ).as_retriever()

    docs = course_data_to_documents(course_data)
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings(api_key=api_key)
    db = FAISS.from_documents(split_docs, embeddings)
    db.save_local(index_dir)
    return db.as_retriever()


# ==============================
# ğŸ§  LangGraph ìƒíƒœ ì •ì˜
# ==============================
class GraphState(TypedDict):
    user_query: str
    final_response: str
    route: Literal["agent1", "agent2"]

# ==============================
# í”„ë¡¬í”„íŠ¸ ë¡œë”©
# ==============================
def load_prompt(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

# ==============================
# ğŸ” ì˜ë„ ë¶„ë¥˜ ì—”ì§„ ë…¸ë“œ
# ==============================
routing_prompt_template = PromptTemplate(
    input_variables=["user_query"],
    template=load_prompt("prompts/routing_prompt.txt")
)

def route_intent(state: GraphState) -> GraphState:
    user_query = state["user_query"]

    formatted_prompt = routing_prompt_template.format(user_query=user_query)

    response = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤."},
            {"role": "user", "content": formatted_prompt}
        ]
    )

    raw = response.choices[0].message.content.strip().lower()

    # ì•ˆì •ì  ì²˜ë¦¬
    if "agent1" in raw:
        route = "agent1"
    elif "agent2" in raw:
        route = "agent2"
    else:
        route = "agent2"  # fallback
    return {**state, "route": route}

# ==============================
# ğŸ¤– Agent1 (RAG ê¸°ë°˜ ì œí’ˆ ë‹µë³€)
# ==============================
agent1_prompt_template = PromptTemplate(
    input_variables=["user_query"],
    template=load_prompt("prompts/agent1_prompt.txt")
)

def agent1_product_info(state: GraphState) -> GraphState:
    try:
        formatted_query = agent1_prompt_template.format(
            user_query=state["user_query"]
        )
        answer = rag_chain.invoke(formatted_query)
    except Exception as e:
        answer = f"â—ì œí’ˆ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    return {**state, "final_response": answer}

# ==============================
# ğŸ“ Agent2 (ê°•ì˜ ì¶”ì²œ ì±—ë´‡)
# ==============================
course_prompt_template = PromptTemplate(
    input_variables=["full_history", "course_data"],
    template=load_prompt("prompts/agent2_prompt.txt")
)
course_retriever = create_course_rag_retriever(course_data)
def agent2_recommend_courses(state: GraphState) -> GraphState:
    full_history = ""
    for turn in st.session_state.chat_history:
        full_history += f"ì‚¬ìš©ì: {turn['user']}\n"
        full_history += f"ì±—ë´‡: {turn['bot']}\n"
    full_history += f"ì‚¬ìš©ì: {state['user_query']}\n"

    try:
        # ìœ ì‚¬ ê°•ì˜ Top N ì¶”ì¶œ
        relevant_docs = course_retriever.invoke(state["user_query"])
        top_courses_text = "\n\n".join(doc.page_content for doc in relevant_docs[:5])

        formatted_prompt = course_prompt_template.format(
            full_history=full_history,
            course_data=top_courses_text
        )

        res = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[
                {"role": "system", "content": "ì‚¼ì„±ì „ì ì„¸ì¼ì¦ˆ ê°•ì˜ ì¶”ì²œ ì „ë¬¸ê°€"},
                {"role": "user", "content": formatted_prompt}
            ]
        )
        response_text = res.choices[0].message.content.strip()
    except Exception as e:
        response_text = f"â—ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

    return {**state, "final_response": response_text}


# ==============================
# ğŸ” LangGraph êµ¬ì¶•
# ==============================
AGENTS = {
    "agent1": agent1_product_info,
    "agent2": agent2_recommend_courses
}
builder = StateGraph(GraphState)
builder.add_node("route_intent", route_intent)

# ì—ì´ì „íŠ¸ ë“±ë¡ ë°˜ë³µë¬¸
for name, func in AGENTS.items():
    builder.add_node(name, func)
builder.set_entry_point("route_intent")

# ì¡°ê±´ë¶€ ë¼ìš°íŒ…ë„ AGENTS ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±
builder.add_conditional_edges("route_intent", lambda x: x["route"], {
    k: k for k in AGENTS
})
graph = builder.compile()


# ==============================
# ğŸ’¾ Supabase ì €ì¥ í•¨ìˆ˜
# ==============================
def save_chat_to_db(user_input, llm_response):
    supabase.table("chat_history").insert({
        "user_id": "guest_user",
        "conversation_id": st.session_state.conversation_id,
        "turn_index": st.session_state.turn_index,
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "user_input": user_input,
        "llm_response": llm_response
    }).execute()
    st.session_state.turn_index += 1

from ui3 import render_app_ui
if __name__ == "__main__":
    render_app_ui(graph, save_chat_to_db)